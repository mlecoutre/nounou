{
  "name": "bench",
  "version": "0.3.4",
  "description": "A little utility for doing side-by-side benchmarks in nodejs",
  "author": {
    "name": "Isaac Z. Schlueter",
    "email": "i@izs.me"
  },
  "bin": {
    "node-bench": "./lib/cli-wrapper.js"
  },
  "main": "lib/bench.js",
  "readme": "# node bench\n\nA little utility for doing side-by-side benchmarks in\n[nodejs](http://nodejs.org).\n\nThis is not for benchmarking your HTTP servers.  Use\n[ab](http://httpd.apache.org/docs/2.0/programs/ab.html) for that.\n\n## Installation\n\n    npm install bench\n\n## Usage\n\nWrite your script like this:\n\n    exports.compare = {\n      \"function wrapper\" : function () {\n        var x = (function (a) {\n          return a;\n        })(\"foo\");\n      },\n      \"with(){} wrapper\" : function () {\n        var x;\n        with ({a : \"foo\"}) {\n          x = a;\n        }\n      }\n      \"no wrapper\" : function () {\n        var a = \"foo\";\n        var x = a;\n      }\n    };\n    require(\"bench\").runMain()\n\nThen, start it up with node.\n\n    $ node my-test-script.js\n\nIt'll output the scores in processes/ms, so a higher score is always better.\nThat is, the values are kHz, not Hz.\n\nYou can also export `time`, `count`, and `comparecount` to change the\nbehavior slightly.\n\nYour test script is just a plain old commonjs module, so it can include other\nthings, update require.paths, whatever setup you need to do.  Generally, it's\na good idea to do this stuff in the module itself, rather than in the\ncomparison functions, so that you can better isolate the units that you\nwant to test.\n\n## Fields\n\nExport the following fields from your benchmark script.\n\n`compare` - The hash of functions that will be compared.  The output will\nuse the object key as the title.  They're called without any arguments, in\nthe default scope.  It's assumed that you should know how to make this do\nwhatever you need it to.\n\n`time` - How long (in ms) to run the tests for.  A higher value will result\nin more accurate tests that take longer to run.  Default: `1000`\n\n`compareCount` - How many times to do the test runs.  This should be some\nfairly small number.  Tests are run multiple times in varying order to\naverage out the variation due to calling one function first, a primed\ncache, garbage collection, etc.  Higher value = more accurate, slower\ntests.  Default: `8`\n\n`countPerLap` - Especially when doing asynchronous benchmarking, you may\nwant to structure your functions so that they run a bunch of times before\ncontinuing.  In these cases, to make your scores reflect the actual number\nof processes per ms, indicate the number of runs per call in the\n\"countPerLap\" field.  Default: `1`\n\n`done` - A function that will be called with the results of the runs\nwhen they're complete.  By default, this calls a function that will\nanalyze the results a bit and write the data to `stdout`.\n\n## Asynchronous Benchmarking\n\nJust write your functions so that they take a single argument.  That\nargument is your callback.  Have fun with it.\n\nYour callback will be fired using `process.nextTick`.  This has a wee\nbit of overhead, so if you're testing something really fast, you should\nprobably construct it to run many times before calling the callback.\nCheck the `examples/nexttick-vs-settimeout.js` test for an example.\n\n# <span style=\"background:red; color:white\">WARNING!</span>\n\nStatistics are powerful tools, and in the wrong hands, can lead to a\nlot of mayhem.  Please use this tool for good, and not evil.\n",
  "readmeFilename": "README.md",
  "_id": "bench@0.3.4",
  "_from": "bench@~0.3.3"
}
